{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#CELL 1\n",
        "#@title Keep this widget playing to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "#@markdown Press play on the audio player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "kaXZpNRbLwlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtBYypQ1CIk0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "import re\n",
        "\n",
        "#@title # **Koboldcpp 1.43 Colab (Improved Edition)**\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # Download Options\n",
        "\n",
        "# URL of the built koboldcpp folder\n",
        "url = \"https://github.com/kalomaze/koboldcpp/releases/download/Colab/koboldcpp.tar.gz\"\n",
        "\n",
        "Model = \"MythoMax-L2-13B-GGUF\" #@param [\"MythoMax-L2-13B-GGUF\", \"ReMM-SLERP-L2-13B-GGUF\", \"Stheno-L2-13B-GGUF\"]\n",
        "Quant_Method = \"4_K_M\" #@param [\"3_K_L\", \"4_K_S\", \"4_K_M\", \"5_K_S\", \"5_K_M\"]\n",
        "\n",
        "#@markdown #### OPTIONAL: Manual Model Link\n",
        "Use_Manual_Model = False #@param {type:\"boolean\"}\n",
        "Manual_Link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # Launch Options\n",
        "\n",
        "\n",
        "Layers = 43 #@param [43]{allow-input: true}\n",
        "Context = 4096 #@param [4096]{allow-input: true}\n",
        "\n",
        "#@markdown #### OPTIONAL: Build Latest Kobold (takes ~7 minutes)\n",
        "Force_Update_Build = False #@param {type:\"boolean\"}\n",
        "\n",
        "model_links = {\n",
        "    \"MythoMax-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q{}.gguf\",\n",
        "    \"ReMM-SLERP-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/ReMM-SLERP-L2-13B-GGUF/resolve/main/remm-slerp-l2-13b.Q{}.gguf\",\n",
        "    \"Stheno-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/Stheno-L2-13B-GGUF/resolve/main/stheno-l2-13b.Q{}.gguf\"\n",
        "}\n",
        "\n",
        "if Use_Manual_Model:\n",
        "    if Manual_Link.strip() != \"\":\n",
        "        print(f\"\\nManual Model detected; will use {Manual_Link} instead of {Model}\\n\")\n",
        "        Model = Manual_Link\n",
        "        Model = Model.replace('/blob/', '/resolve/')\n",
        "    else:\n",
        "        print(f\"\\nWarning: Manual Model enabled, but no link was found. Falling back to {Model}\\n\")\n",
        "        if Model in model_links:\n",
        "            Model = model_links[Model].format(Quant_Method)\n",
        "else:\n",
        "    if Model in model_links:\n",
        "        Model = model_links[Model].format(Quant_Method)\n",
        "\n",
        "# Check if Model doesn't end in the specified formats\n",
        "if not re.search(r'(\\.gguf|\\.ggml|\\.bin|\\.safetensors)$', Model):\n",
        "    print(\"--------------------------\\n5 SECOND WARNING: Manual link provided doesn't end with a supported format.\\nAre you sure you provided a direct link?\\n--------------------------\\n\")\n",
        "    !sleep 5\n",
        "\n",
        "# Check if the Model starts with https://huggingface.co/ but doesn't follow the specified format\n",
        "if Model.startswith('https://huggingface.co/') and not re.search(r'^https://huggingface\\.co/.+/.+/.+/.+/[^/]+\\.[^/]+$', Model):\n",
        "    print(\"--------------------------\\n10 SECOND WARNING: The HuggingFace link provided is of the entire model repository.\\nPlease find the direct link to the quant you want to use.\\n--------------------------\\n\")\n",
        "    !sleep 10\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists('/content/koboldcpp/'):\n",
        "    os.makedirs('/content/koboldcpp/')\n",
        "\n",
        "# Checking if you already downloaded Kobold\n",
        "if not os.path.exists(\"/content/koboldcpp.tar.gz\"):\n",
        "    if Force_Update_Build == False:\n",
        "        response = requests.get(url, stream=True)\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        with open(filename, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "\n",
        "        with tarfile.open(filename, 'r:gz') as tar:\n",
        "            for member in tar.getmembers():\n",
        "                if member.name.startswith('koboldcpp'):\n",
        "                    try:\n",
        "                        tar.extract(member, path='/content')\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error extracting '{member.name}': {str(e)}\")\n",
        "\n",
        "        print(\"Kobold extraction to /content/ completed!\")\n",
        "    else:\n",
        "        print(\"Skipping prebuilt kobold, will build manually...\")\n",
        "        !git clone https://github.com/LostRuins/koboldcpp\n",
        "        %cd /content/koboldcpp\n",
        "        !make LLAMA_CUBLAS=1\n",
        "\n",
        "# Change to the directory\n",
        "%cd /content/koboldcpp\n",
        "\n",
        "# Hosting the cloudflared server\n",
        "!wget -c -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!echo > nohup.out\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:5001 &\n",
        "!sleep 10\n",
        "!cat nohup.out\n",
        "\n",
        "# Download the file if it doesn't exist already\n",
        "if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !wget $Model -O model.gguf\n",
        "\n",
        "if os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !rm koboldcpp.py\n",
        "    !wget https://github.com/kalomaze/koboldcpp/raw/colab-api-url/koboldcpp.py\n",
        "    !python koboldcpp.py model.gguf --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo\n",
        "else:\n",
        "    print(\"Failed to download the GGUF model. Please retry.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick How-To Guide\n",
        "\n",
        "---\n",
        "## Step 1. Keeping Google Colab Running\n",
        "---\n",
        "\n",
        "Google Colab has a tendency to timeout after a period of inactivity. If you want to ensure your session doesn't timeout abruptly, you can use the following widget.\n",
        "\n",
        "### Starting the Widget for Audio Player:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363694191104112/image.png\" width=\"50%\"/>\n",
        "\n",
        "### How the Widget Looks When Playing:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363653997076540/image.png\" width=\"50%\"/>\n",
        "\n",
        "Follow the visual cues in the images to start the widget and ensure that the notebook remains active.\n",
        "\n",
        "---\n",
        "## Step 2. Decide your Model\n",
        "---\n",
        "\n",
        "Pick a model and the quantization from the dropdowns, then run the cell like how you did earlier.\n",
        "\n",
        "### Select your Model and Quantization:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150370141557764106/image.png\" width=\"40%\"/>\n",
        "\n",
        "Alternatively, you can specify a model manually.\n",
        "\n",
        "### Manual Model Option:\n",
        "\n",
        "> <img src=\"https://media.discordapp.net/attachments/945486970883285045/1150370631242764370/image.png\" width=\"75%\"/>\n",
        "\n",
        "5_K_M 13b models should work with 4k (maybe 3k?) context on Colab, since the T4 GPU has ~16GB of VRAM. You can now start the cell, and after 1-3 minutes, it should end with your API link that you can connect to in [SillyTavern](https://docs.sillytavern.app/installation/windows/):\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150438287882862674/image.png\" width=\"80%\"/>\n",
        "\n",
        "---\n",
        "# And there you have it!\n",
        "### MythoMax (or any 7b / 13b Llama 2 model) in under 2 minutes.\n",
        "#### (depending on whether or not huggingface downloads are experiencing high traffic)\n",
        "\n",
        "---\n",
        "\n",
        "# Credits\n",
        "### - Made with ~~spite~~ love by kalomaze ❤️ <sub>(also here's the part where I shill my [Patreon](https://www.patreon.com/kalomaze) if you care!)</sub>\n",
        "### - Koboldcpp is not my software, this is just to make it easy to use on Colab. You can find the original GitHub repository for it here: https://github.com/LostRuins/koboldcpp"
      ],
      "metadata": {
        "id": "zaQl3v9wali-"
      }
    }
  ]
}