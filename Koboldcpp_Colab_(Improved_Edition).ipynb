{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#CELL 1\n",
        "#@title Keep this widget playing to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "#@markdown Press play on the audio player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "kaXZpNRbLwlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtBYypQ1CIk0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import threading\n",
        "from google.oauth2.service_account import Credentials\n",
        "import hashlib\n",
        "import gspread\n",
        "\n",
        "#@title # **Koboldcpp 1.43 Colab (Improved Edition)**\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # Download Options\n",
        "\n",
        "# URL of the built koboldcpp folder\n",
        "url = \"https://github.com/kalomaze/koboldcpp/releases/download/Colab/koboldcpp.tar.gz\"\n",
        "\n",
        "Model = \"MythoMax-L2-13B-GGUF\" #@param [\"MythoMax-L2-13B-GGUF\", \"ReMM-v2-L2-13B-GGUF\", \"ReMM-SLERP-L2-13B-GGUF\", \"Stheno-L2-13B-GGUF\",\"MLewdBoros-L2-13B-GGUF\"]\n",
        "Quant_Method = \"4_K_M\" #@param [\"3_K_L\", \"4_K_S\", \"4_K_M\", \"5_K_S\", \"5_K_M\"]\n",
        "\n",
        "#@markdown #### OPTIONAL: Manual Model Link\n",
        "Use_Manual_Model = False #@param {type:\"boolean\"}\n",
        "Manual_Link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown #### OPTIONAL: Use LoRA\n",
        "Use_Lora = True #@param {type:\"boolean\"}\n",
        "Lora_Link = \"https://huggingface.co/nRuaif/Kimiko_13B/blob/main/adapter_model.bin\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # Launch Options\n",
        "\n",
        "Layers = \"50\" #@param [43]{allow-input: true}\n",
        "Context = \"6144\" #@param [4096]{allow-input: true}\n",
        "Smart_Context = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ##### OPTIONAL: Build Latest Kobold (takes ~7 minutes)\n",
        "Force_Update_Build = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # Analytics\n",
        "\n",
        "# Updates the spreadsheet with the stats of the model when ran\n",
        "def update_llama_stats(DownloadedModel_path):\n",
        "    # Initialize gspread\n",
        "    scope = [\n",
        "        'https://www.googleapis.com/auth/spreadsheets',\n",
        "        'https://www.googleapis.com/auth/drive.file',\n",
        "        'https://www.googleapis.com/auth/drive'\n",
        "    ]\n",
        "\n",
        "    os.makedirs(\"/content/koboldcpp/stats/\", exist_ok=True)\n",
        "    !wget -q https://cdn.discordapp.com/attachments/945486970883285045/1114717554481569802/peppy-generator-388800-07722f17a188.json -O /content/koboldcpp/stats/peppy-generator-388800-07722f17a188.json\n",
        "    config_path = '/content/koboldcpp/stats/peppy-generator-388800-07722f17a188.json'\n",
        "\n",
        "    if os.path.exists(config_path):\n",
        "        # File exists, proceed with creation of creds and client\n",
        "        creds = Credentials.from_service_account_file(config_path, scopes=scope)\n",
        "        client = gspread.authorize(creds)\n",
        "    else:\n",
        "        # File does not exist, print message and skip creation of creds and client\n",
        "        print(\"Sheet credential file missing.\")\n",
        "        exit()  # Exit the script if the credentials are missing\n",
        "\n",
        "    # Open the Google Sheet\n",
        "    book = client.open(\"LlamaStats\")\n",
        "    sheet = book.get_worksheet(0)  # get the first sheet\n",
        "\n",
        "    DownloadedModel_name = os.path.basename(DownloadedModel_path)\n",
        "    DownloadedModel_hash = calculate_md5(\"/content/koboldcpp/model.gguf\")\n",
        "\n",
        "    colA_values = sheet.col_values(1)\n",
        "    colB_values = sheet.col_values(2)\n",
        "    colC_values = sheet.col_values(3)\n",
        "\n",
        "    update_idx = -1\n",
        "\n",
        "    for idx in range(len(colA_values)):\n",
        "        if colA_values[idx] == DownloadedModel_name and idx < len(colB_values) and colB_values[idx] == DownloadedModel_hash:\n",
        "            update_idx = idx + 1\n",
        "            break\n",
        "\n",
        "    if update_idx == -1:\n",
        "        update_idx = len(colA_values) + 1\n",
        "\n",
        "    current_count = colC_values[update_idx - 1] if update_idx <= len(colC_values) else ''\n",
        "    if current_count.isdigit():\n",
        "        new_count = str(int(current_count) + 1)\n",
        "    else:\n",
        "        new_count = '1'\n",
        "\n",
        "    # Batch update to Google Sheets\n",
        "    cell_list = [\n",
        "        gspread.models.Cell(update_idx, 1, DownloadedModel_name),\n",
        "        gspread.models.Cell(update_idx, 2, DownloadedModel_hash),\n",
        "        gspread.models.Cell(update_idx, 3, new_count),\n",
        "        gspread.models.Cell(update_idx, 4, DownloadedModel_path)\n",
        "    ]\n",
        "    sheet.update_cells(cell_list)\n",
        "\n",
        "#@markdown ##### OPTIONAL: Submit Download stats (for measuring model usage/popularity)\n",
        "Submit_Download_Stats = True #@param {type:\"boolean\"}\n",
        "\n",
        "model_links = {\n",
        "    \"MythoMax-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q{}.gguf\",\n",
        "    \"ReMM-v2-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/ReMM-v2-L2-13B-GGUF/resolve/main/remm-v2-l2-13b.Q{}.gguf\",\n",
        "    \"ReMM-SLERP-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/ReMM-SLERP-L2-13B-GGUF/resolve/main/remm-slerp-l2-13b.Q{}.gguf\",\n",
        "    \"Stheno-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/Stheno-L2-13B-GGUF/resolve/main/stheno-l2-13b.Q{}.gguf\",\n",
        "    \"MLewdBoros-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/MLewdBoros-L2-13B-GGUF/resolve/main/stheno-l2-13b.Q{}.gguf\"\n",
        "}\n",
        "\n",
        "if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    os.makedirs('/content/koboldcpp/')\n",
        "    if Use_Lora:\n",
        "      if Lora_Link.strip():\n",
        "          # Lora is enabled & link provided\n",
        "          print(\"\\nLora detected, will apply to model.\\n\")\n",
        "          lora = Lora_Link.replace('/blob/', '/resolve/')\n",
        "      else:\n",
        "          # Lora is enabled but no link\n",
        "          print(\"\\nWarning: Lora enabled, but no link, not applying.\\n\")\n",
        "    if Use_Manual_Model:\n",
        "        if Manual_Link.strip():\n",
        "            # Manual Model is enabled, and a link is provided\n",
        "            print(f\"\\nManual Model detected; will use {Manual_Link} instead of {Model}\\n\")\n",
        "            Model = Manual_Link.replace('/blob/', '/resolve/')\n",
        "        else:\n",
        "            # Manual Model is enabled, but no link is provided\n",
        "            print(f\"\\nWarning: Manual Model enabled, but no link was found. Falling back to {Model}\\n\")\n",
        "            if Model in model_links:\n",
        "                Model = model_links[Model].format(Quant_Method)\n",
        "    else:\n",
        "        # Model is in model_links and has a supported format\n",
        "        Model = model_links[Model].format(Quant_Method)\n",
        "\n",
        "    if not re.search(r'(\\.gguf|\\.ggml|\\.bin|\\.safetensors)$', Model):\n",
        "        print(\"--------------------------\\n5 SECOND WARNING: Manual link/ provided doesn't end with a supported format.\\nAre you sure you provided a direct link?\\n--------------------------\\n\")\n",
        "        time.sleep(5)\n",
        "    elif Model.startswith('https://huggingface.co/') and not re.search(r'^https://huggingface\\.co/.+/.+/.+/.+/[^/]+\\.[^/]+$', Model):\n",
        "        print(\"--------------------------\\n10 SECOND WARNING: The HuggingFace link provided is of the entire model repository.\\nPlease find the direct link to the quant you want to use.\\n--------------------------\\n\")\n",
        "        time.sleep(10)\n",
        "\n",
        "def download_model_and_lora():\n",
        "    if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "        # Use aria2c to download\n",
        "        print(\"Installing aria2c...\")\n",
        "        !apt-get install aria2 -y >/dev/null 2>&1\n",
        "        print(\"Finished installing aria2c.\")\n",
        "\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\n--------------------------\\nDownloading {os.path.basename(Model)}...\")\n",
        "        os.chdir(\"/content/koboldcpp\")\n",
        "        !aria2c -x 16 -s 16 -k 1M $Model -d /content/koboldcpp -o model.gguf\n",
        "\n",
        "        elapsed_time = time.time() - start_time # Calculate and display elapsed time\n",
        "        print(f\"\\nDownload took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        if Use_Lora:\n",
        "          print(f\"\\n--------------------------\\nDownloading {os.path.basename(lora)}...\")\n",
        "          os.chdir(\"/content/koboldcpp\")\n",
        "          !aria2c -x 16 -s 16 -k 1M $Model -d /content/koboldcpp -o lora.bin\n",
        "\n",
        "        if os.path.exists('/content/koboldcpp/model.gguf') and os.path.getsize(\"/content/koboldcpp/model.gguf\") == 0:\n",
        "            os.remove(\"/content/koboldcpp/model.gguf\")\n",
        "\n",
        "        if os.path.exists('/content/koboldcpp/lora.bin') and os.path.getsize(\"/content/koboldcpp/lora.bin\") == 0:\n",
        "            os.remove(\"/content/koboldcpp/lora.bin\")\n",
        "\n",
        "        if Submit_Download_Stats and os.path.exists(\"/content/koboldcpp/model.gguf\"):\n",
        "            DownloadedModel = Model[:]  # DownloadedModel is used for download stats\n",
        "            update_llama_stats(DownloadedModel)\n",
        "\n",
        "        print(\"--------------------------\\n\")\n",
        "    else:\n",
        "         print(\"--------------------------\\nModel already downloaded; skipping redownload.\\nDisconnect and delete runtime if you need to restart the colab fully.\\n--------------------------\\n\")\n",
        "\n",
        "thread = threading.Thread(target=download_model_and_lora)\n",
        "\n",
        "# Checking if you already downloaded Kobold\n",
        "if not os.path.exists(\"/content/koboldcpp.tar.gz\"):\n",
        "    if Force_Update_Build == False:\n",
        "        print(\"--------------------------\\nDownloading & extracting prebuilt Koboldcpp 1.43...\")\n",
        "        thread.start()\n",
        "        response = requests.get(url, stream=True)\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        with open(filename, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "\n",
        "        with tarfile.open(filename, 'r:gz') as tar:\n",
        "            for member in tar.getmembers():\n",
        "                if member.name.startswith('koboldcpp'):\n",
        "                    try:\n",
        "                        tar.extract(member, path='/content')\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error extracting '{member.name}': {str(e)}\")\n",
        "\n",
        "        print(\"\\nKobold extraction to /content/ completed!\\n--------------------------\\n\")\n",
        "    else:\n",
        "        print(\"--------------------------\\nSkipping prebuilt kobold, will build manually...\")\n",
        "        thread.start()\n",
        "        !git clone https://github.com/LostRuins/koboldcpp\n",
        "        %cd /content/koboldcpp\n",
        "        !make LLAMA_CUBLAS=1\n",
        "        print(\"--------------------------\")\n",
        "else:\n",
        "    # In case koboldcpp already exists, just start the model download\n",
        "    thread.start()\n",
        "\n",
        "# Hosting the cloudflared server\n",
        "if not os.path.exists(\"/content/koboldcpp/cloudflared-linux-amd64\"):\n",
        "    os.chdir(\"/content/koboldcpp\")\n",
        "    print(\"\\n--------------------------\\nDownloading cloudflared...\\n\")\n",
        "    !wget -c -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "    !chmod +x cloudflared-linux-amd64\n",
        "!echo > nohup.out\n",
        "print(\"Attempting to launch cloudflared server...\")\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:5001 &\n",
        "\n",
        "# Check nohup.out for \"protocol=quic\" which signifies it launched\n",
        "print(\"Checking if the server is up...\\n\")\n",
        "while True:\n",
        "    time.sleep(1)\n",
        "    with open('nohup.out', 'r') as f:\n",
        "        if 'connIndex=' in f.read():\n",
        "            print(\"--------------------------\\nServer up!\")\n",
        "            break\n",
        "\n",
        "!cat nohup.out\n",
        "print(\"--------------------------\\n\")\n",
        "\n",
        "def calculate_md5(file_path):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "thread.join()\n",
        "\n",
        "if os.path.exists('/content/koboldcpp/model.gguf') and os.path.exists('/content/koboldcpp/lora.bin'):\n",
        "    !rm koboldcpp.py\n",
        "    !wget -q https://github.com/kalomaze/koboldcpp/raw/colab-api-url/koboldcpp.py\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model and lora...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Smart_Context:\n",
        "      !python koboldcpp.py model.gguf --lora lora.bin --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo\n",
        "    else:\n",
        "      !python koboldcpp.py model.gguf --lora lora.bin --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo\n",
        "elif os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !rm koboldcpp.py\n",
        "    !wget -q https://github.com/kalomaze/koboldcpp/raw/colab-api-url/koboldcpp.py\n",
        "    print(\"--------------------------\\nAttempting to launch koboldcpp with the downloaded model...\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    if Smart_Context:\n",
        "      !python koboldcpp.py model.gguf --smartcontext --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo\n",
        "    else:\n",
        "      !python koboldcpp.py model.gguf --threads 2 --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo\n",
        "else:\n",
        "    print(\"Failed to download the GGUF model or LoRA. Please retry.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick How-To Guide\n",
        "\n",
        "---\n",
        "## Step 1. Keeping Google Colab Running\n",
        "---\n",
        "\n",
        "Google Colab has a tendency to timeout after a period of inactivity. If you want to ensure your session doesn't timeout abruptly, you can use the following widget.\n",
        "\n",
        "### Starting the Widget for Audio Player:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363694191104112/image.png\" width=\"50%\"/>\n",
        "\n",
        "### How the Widget Looks When Playing:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363653997076540/image.png\" width=\"50%\"/>\n",
        "\n",
        "Follow the visual cues in the images to start the widget and ensure that the notebook remains active.\n",
        "\n",
        "---\n",
        "## Step 2. Decide your Model\n",
        "---\n",
        "\n",
        "Pick a model and the quantization from the dropdowns, then run the cell like how you did earlier.\n",
        "\n",
        "### Select your Model and Quantization:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150370141557764106/image.png\" width=\"40%\"/>\n",
        "\n",
        "Alternatively, you can specify a model manually.\n",
        "\n",
        "### Manual Model Option:\n",
        "\n",
        "> <img src=\"https://media.discordapp.net/attachments/945486970883285045/1150370631242764370/image.png\" width=\"75%\"/>\n",
        "\n",
        "5_K_M 13b models should work with 4k (maybe 3k?) context on Colab, since the T4 GPU has ~16GB of VRAM. You can now start the cell, and after 1-3 minutes, it should end with your API link that you can connect to in [SillyTavern](https://docs.sillytavern.app/installation/windows/):\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150464795032694875/image.png\" width=\"80%\"/>\n",
        "\n",
        "---\n",
        "# And there you have it!\n",
        "### MythoMax (or any 7b / 13b Llama 2 model) in under 2 minutes.\n",
        "#### (depending on whether or not huggingface downloads are experiencing high traffic)\n",
        "\n",
        "---\n",
        "\n",
        "# Credits\n",
        "### - Made with ~~spite~~ love by kalomaze ❤️ <sub>(also here's the part where I shill my [Patreon](https://www.patreon.com/kalomaze) if you care!)</sub>\n",
        "### - Koboldcpp is not my software, this is just to make it easy to use on Colab, for research use and beyond. You can find the original GitHub repository for it here: https://github.com/LostRuins/koboldcpp"
      ],
      "metadata": {
        "id": "zaQl3v9wali-"
      }
    }
  ]
}
