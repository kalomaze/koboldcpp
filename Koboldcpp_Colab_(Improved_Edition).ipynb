{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#CELL 1\n",
        "#@title Keep this widget playing to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "#@markdown Press play on the audio player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "kaXZpNRbLwlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtBYypQ1CIk0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import threading\n",
        "import time\n",
        "\n",
        "#@title # **Koboldcpp Colab (Improved Edition)**\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # Download Options\n",
        "\n",
        "\n",
        "# URL of the built koboldcpp folder\n",
        "url = \"https://github.com/kalomaze/koboldcpp/releases/download/Colab/koboldcpp.tar.gz\"\n",
        "\n",
        "Model = \"MythoMax-L2-13B-GGUF\" #@param [\"MythoMax-L2-13B-GGUF\", \"ReMM-SLERP-L2-13B-GGUF\", \"Stheno-L2-13B-GGUF\"]\n",
        "Quant_Method = \"4_K_M\" #@param [\"3_K_L\", \"4_K_S\", \"4_K_M\", \"5_K_S\", \"5_K_M\"]\n",
        "\n",
        "#@markdown ### OPTIONAL: Manual Model Link\n",
        "Use_Manual_Model = False #@param {type:\"boolean\"}\n",
        "Manual_Link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # Launch Options\n",
        "\n",
        "\n",
        "Layers = 43 #@param [43]{allow-input: true}\n",
        "Context = 4096 #@param [4096]{allow-input: true}\n",
        "\n",
        "#@markdown ### OPTIONAL: Build Latest Kobold (takes ~7 minutes)\n",
        "Force_Update_Build = False #@param {type:\"boolean\"}\n",
        "\n",
        "model_links = {\n",
        "    \"MythoMax-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q{}.gguf\",\n",
        "    \"ReMM-SLERP-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/ReMM-SLERP-L2-13B-GGUF/resolve/main/remm-slerp-l2-13b.Q{}.gguf\",\n",
        "    \"Stheno-L2-13B-GGUF\": \"https://huggingface.co/TheBloke/Stheno-L2-13B-GGUF/resolve/main/stheno-l2-13b.Q{}.gguf\"\n",
        "}\n",
        "\n",
        "if Use_Manual_Model:\n",
        "    if Manual_Link.strip() != \"\":\n",
        "        print(f\"\\nManual Model detected; will use {Manual_Link} instead of {Model}\\n\")\n",
        "        Model = Manual_Link\n",
        "    else:\n",
        "        print(f\"\\nWarning: Manual Model enabled, but no link was found. Falling back to {Model}\\n\")\n",
        "        if Model in model_links:\n",
        "            Model = model_links[Model].format(Quant_Method)\n",
        "else:\n",
        "    if Model in model_links:\n",
        "        Model = model_links[Model].format(Quant_Method)\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists('/content/koboldcpp/'):\n",
        "    os.makedirs('/content/koboldcpp/')\n",
        "\n",
        "# Checking if you already downloaded Kobold\n",
        "if not os.path.exists(\"/content/koboldcpp.tar.gz\"):\n",
        "    if Force_Update_Build == False:\n",
        "        response = requests.get(url, stream=True)\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        with open(filename, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "\n",
        "        with tarfile.open(filename, 'r:gz') as tar:\n",
        "            for member in tar.getmembers():\n",
        "                if member.name.startswith('koboldcpp'):\n",
        "                    try:\n",
        "                        tar.extract(member, path='/content')\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error extracting '{member.name}': {str(e)}\")\n",
        "\n",
        "        print(\"Kobold extraction to /content/ completed!\")\n",
        "    else:\n",
        "        print(\"Skipping prebuilt kobold, will build manually...\")\n",
        "        !git clone https://github.com/LostRuins/koboldcpp\n",
        "        %cd /content/koboldcpp\n",
        "        !make LLAMA_CUBLAS=1\n",
        "\n",
        "# Change to the directory\n",
        "%cd /content/koboldcpp\n",
        "\n",
        "# Hosting the cloudflared server\n",
        "!wget -c -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!echo > nohup.out\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:5001 &\n",
        "!sleep 10\n",
        "!cat nohup.out\n",
        "\n",
        "# Download the file if it doesn't exist already\n",
        "if not os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !wget $Model -O model.gguf\n",
        "\n",
        "if os.path.exists('/content/koboldcpp/model.gguf'):\n",
        "    !rm koboldcpp.py\n",
        "    !wget https://cdn.discordapp.com/attachments/1132396046480314388/1150324006407376927/koboldcpp.py\n",
        "    !python koboldcpp.py model.gguf --stream --usecublas 0 normal mmq --context $Context --ropeconfig 1.0 10000 --gpulayers $Layers --hordeconfig concedo\n",
        "else:\n",
        "    print(\"Failed to download the GGUF model. Please retry.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick How-To Guide\n",
        "\n",
        "---\n",
        "## Step 1. Keeping Google Colab Running\n",
        "---\n",
        "\n",
        "Google Colab has a tendency to timeout after a period of inactivity. If you want to ensure your session doesn't timeout abruptly, you can use the following widget.\n",
        "\n",
        "### Starting the Widget for Audio Player:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363694191104112/image.png\" width=\"50%\"/>\n",
        "\n",
        "### How the Widget Looks When Playing:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150363653997076540/image.png\" width=\"50%\"/>\n",
        "\n",
        "Follow the visual cues in the images to start the widget and ensure that the notebook remains active.\n",
        "\n",
        "---\n",
        "## Step 2. Decide your Model\n",
        "---\n",
        "\n",
        "Pick a model and the quantization from the dropdowns, then run the cell like how you did earlier.\n",
        "\n",
        "### Select your Model and Quantization:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150370141557764106/image.png\" width=\"40%\"/>\n",
        "\n",
        "Alternatively, you can specify a model manually.\n",
        "\n",
        "### Manual Model Option:\n",
        "\n",
        "> <img src=\"https://media.discordapp.net/attachments/945486970883285045/1150370631242764370/image.png\" width=\"75%\"/>\n",
        "\n",
        "5_K_M 13b models should work with 4k (maybe 3k?) context on Colab, since the T4 GPU has ~16GB of VRAM. You can now start the cell, and after 1-3 minutes, it should end with your API link that you can connect to in SillyTavern:\n",
        "\n",
        "> <img src=\"https://cdn.discordapp.com/attachments/945486970883285045/1150372328644018287/image.png\" width=\"80%\"/>\n",
        "\n",
        "---\n",
        "# And there you have it!\n",
        "### MythoMax (or any 7b / 13b Llama 2 model) in under 2 minutes.\n",
        "#### (depending on whether or not huggingface downloads are experiencing high traffic)"
      ],
      "metadata": {
        "id": "zaQl3v9wali-"
      }
    }
  ]
}